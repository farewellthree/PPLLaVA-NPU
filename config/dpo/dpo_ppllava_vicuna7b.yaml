model:
  arch: llava_interleave
  llama_model: llava-hf/llava-v1.6-vicuna-7b-hf
  gradient_checkpointing: True
  pooling: ppllava
  clip_weight: openai/clip-vit-large-patch14-336
  pooling_kernel: (2,3,3)
  pooling_stride: (2,3,3)
  image_pooling_kernel: (1,3,3)
  image_pooling_stride: (1,3,3)
  frame_shape: (24,24)
  btadapter: True
  extend_clip: True
  freeze_LLM: False
  pad_token_id: 0
  onlyLLM: True
  ckpt: /Path/to/ppllava_vicuna7b_image_video_ckpt


datasets:
  {}

run:
  task: video_text_it
  bf16: True
  fp16: False
  tf32: False
  output_dir: "./ppllava/output/dpo/dpo_ppllava_vicuna7b"
  version: 'v1'
  dpo_alpha: 1.0
  beta: 0.1
  gamma: 0
  training_modal: 'video'
  data_path: 'Path/to/sft_dpo_17k.jsonl'
  video_folder: 'Path/to/llava-hound-300k'
  image_folder: '/'
  image_aspect_ratio: 'pad'
  group_by_modality_length: False
  lazy_preprocess: True
  num_train_epochs: 2
  dataloader_num_workers: 4
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  evaluation_strategy: "no"
  learning_rate: 5e-6
  weight_decay: 0.
  warmup_ratio: 0.1
  lr_scheduler_type: 'linear'
  logging_steps: 1
  model_max_length: 2048
  save_strategy: "steps" 
  save_steps: 40
  save_total_limit: 5
  save_only_model: True 
  deepspeed: 'ppllava/train/zero2.json'
  num_frames: 32